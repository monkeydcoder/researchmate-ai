# ResearchMate AI

ğŸš€ A privacy-focused, locally-run research assistant powered by Ollama & Llama 3.

---

## ğŸ¥ Demo Video

[Watch the walkthrough on Loom](https://www.loom.com/share/f0cf23e5e03546e5875df76d9d26199b?sid=43511f12-3fcf-43cb-bfdd-4964c7713bf9)

---

Summarize papers, extract insights, generate citations, brainstorm research questions, and moreâ€”all on your own machine! ğŸ§‘â€ğŸ’»ğŸ“„ğŸ”’

---

## ğŸš€ Features

- **Paper Summarization:** Turn lengthy research papers into concise, readable summaries
- **Key Insights Extraction:** Identify and extract the most important findings and contributions
- **Research Trends:** Explore the latest developments in your field with AI-curated trends
- **Citation Generator:** Create properly formatted citations (APA, MLA, Chicago, etc.)
- **Research Questions Generator:** Brainstorm focused, researchable questions for your topic
- **Literature Review Helper:** Organize and synthesize papers into a coherent review
- **Privacy-Focused:** All processing happens locally using Ollama and Llama 3â€”your data never leaves your computer

---

## ğŸ—ï¸ Architecture

```
User (Web Browser)
    â”‚
    â–¼
Frontend (React, Tailwind CSS)
    â”‚  (HTTP requests)
    â–¼
Backend (Node.js, Express)
    â”‚  (API calls)
    â–¼
Ollama (Llama 3 model runs locally)
```

- **Frontend:** http://localhost:3000
- **Backend:** http://localhost:3001
- **Ollama API:** http://localhost:11434

---

## ğŸ› ï¸ Prerequisites

- [Node.js](https://nodejs.org/) (v14 or higher)
- [npm](https://www.npmjs.com/) (v6 or higher)
- [Ollama](https://ollama.ai/) installed on your machine
- Llama 3 model pulled in Ollama

---

## âš¡ Quick Start

### 1. Clone the repository
```bash
git clone https://github.com/yourusername/research-assistant.git
cd research-assistant
```

### 2. Install dependencies
```bash
# Backend
yarn install # or npm install (in research-assistant/backend)
cd backend
npm install

# Frontend
cd ../frontend
npm install
```

### 3. Set up Ollama and Llama 3
```bash
ollama serve
ollama pull llama3
```

### 4. Start the backend (port 3001)
```bash
cd backend
PORT=3001 node server.js
```

### 5. Start the frontend (port 3000)
```bash
cd ../frontend
npm start
```

### 6. Open your browser
Go to [http://localhost:3000](http://localhost:3000)

---

## ğŸ§‘â€ğŸ’» Usage Guide

- **Paper Analysis:**
  - Go to "Paper Analysis"
  - Paste your research paper text
  - Choose summary, key insights, or both
  - Get instant AI-powered results

- **Research Trends:**
  - Go to "Research Trends"
  - Enter a topic (e.g., "transformer models")
  - Get a summary of recent developments

- **Citation Generator:**
  - Go to "Citation Generator"
  - Enter paper details and select format
  - Get a ready-to-use citation

- **Research Questions Generator:**
  - Go to "Research Questions"
  - Enter your topic and context
  - Get focused, researchable questions

- **Literature Review Helper:**
  - Go to "Literature Review"
  - Add papers and a research topic
  - Get a structured literature review draft

---

## ğŸ—‚ï¸ Project Structure

```
research-assistant/
â”œâ”€â”€ backend/               # Node.js + Express backend
â”‚   â”œâ”€â”€ server.js          # Main server file
â”‚   â””â”€â”€ package.json       # Backend dependencies
â”œâ”€â”€ frontend/              # React frontend
â”‚   â”œâ”€â”€ public/            # Static files
â”‚   â”œâ”€â”€ src/               # Source files
â”‚   â”‚   â”œâ”€â”€ components/    # Reusable components
â”‚   â”‚   â”œâ”€â”€ pages/         # Page components
â”‚   â”‚   â”œâ”€â”€ services/      # API services
â”‚   â”‚   â””â”€â”€ utils/         # Utility functions
â”‚   â””â”€â”€ package.json       # Frontend dependencies
â”œâ”€â”€ docs/                  # Additional documentation
â”‚   â”œâ”€â”€ API.md             # API documentation
â”‚   â””â”€â”€ Ollama-Setup.md    # Ollama setup guide
â”œâ”€â”€ start-all.sh           # Script to start both servers (macOS)
â”œâ”€â”€ README.md              # Project documentation
â””â”€â”€ QUICK-START.md         # Quick start guide
```

---

## ğŸ§© Technology Stack

- **Frontend:** React, Tailwind CSS, React Router
- **Backend:** Node.js, Express
- **AI Processing:** Ollama, Llama 3

---

## ğŸ›Ÿ Troubleshooting & FAQ

- **Ollama not running?**
  - Make sure you started it with `ollama serve`
- **Model not found?**
  - Run `ollama pull llama3` to download the model
- **Port already in use?**
  - Change the backend port in `backend/server.js` or use `PORT=xxxx node server.js`
- **Frontend not connecting?**
  - Ensure backend is running on port 3001 and Ollama is running
- **Slow responses?**
  - Large papers or limited hardware may slow down processing

---

## ğŸŒ Deploying to GitHub

1. **Initialize git (if not already):**
   ```bash
   git init
   git add .
   git commit -m "Initial commit"
   ```
2. **Create a new repo on GitHub**
3. **Add remote and push:**
   ```bash
   git remote add origin https://github.com/<your-username>/<repo-name>.git
   git branch -M main
   git push -u origin main
   ```

---

## ğŸ¤ Contributing

Pull requests are welcome! For major changes, please open an issue first to discuss what you would like to change.

---

## ğŸ“„ License

This project is licensed under the MIT License - see the LICENSE file for details.

---

## ğŸ™ Acknowledgements

- [Ollama](https://ollama.ai/) for local LLM serving
- [Meta AI](https://ai.meta.com/) for Llama 3
- All contributors and open source libraries used 