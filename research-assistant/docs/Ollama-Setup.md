# Ollama Setup Guide for ResearchMate AI

This guide will help you set up Ollama and the necessary models to use with ResearchMate AI.

## What is Ollama?

Ollama is an open-source tool that allows you to run large language models (LLMs) locally on your machine. It provides easy access to powerful AI models without sending your data to external services.

## Installation

### macOS

1. Download Ollama from the [official website](https://ollama.ai/download).
2. Open the downloaded file and follow the installation instructions.
3. After installation, Ollama will run in the background.

### Linux

```bash
curl -fsSL https://ollama.ai/install.sh | sh
```

### Windows

Windows support is available through WSL (Windows Subsystem for Linux).

1. Install WSL following [Microsoft's guide](https://learn.microsoft.com/en-us/windows/wsl/install).
2. Inside WSL, run: `curl -fsSL https://ollama.ai/install.sh | sh`

## Running Ollama

Once installed, you can start Ollama with:

```bash
ollama serve
```

This will start the Ollama server, which listens on `localhost:11434` by default.

## Installing Models

For ResearchMate AI, we recommend using the Llama 3 model. You can pull it with:

```bash
ollama pull llama3
```

For even better performance, you can use the larger version if your hardware supports it:

```bash
ollama pull llama3:8b  # 8 billion parameters
# or
ollama pull llama3:70b  # 70 billion parameters (requires more RAM)
```

## Verifying Installation

You can verify that Ollama is working correctly by running a simple test:

```bash
ollama run llama3 "What is a language model?"
```

You should see a response generated by the model.

## Configuring ResearchMate AI to Use Ollama

ResearchMate AI is already configured to use Ollama running on the default port (11434). If your Ollama instance is running on a different port or host, you'll need to update the `.env` file in the backend directory:

```
PORT=5000
OLLAMA_API_URL=http://localhost:11434/api
```

## Advanced Ollama Configuration

### Custom Model Configuration

You can create custom versions of models with specific parameters by creating a `Modelfile`. For example:

```
# Create a file named Modelfile
FROM llama3
PARAMETER temperature 0.7
PARAMETER top_p 0.9
PARAMETER top_k 40

# Build the custom model
ollama create research-assistant-llm -f Modelfile

# Use it in the app by changing the model name in the requests
```

### Memory and Performance

For better performance with larger papers, you may want to allocate more memory to Ollama. You can do this by setting the `OLLAMA_HOST_MEMORY` environment variable before starting Ollama:

```bash
OLLAMA_HOST_MEMORY=8192 ollama serve  # Allocate 8GB of RAM
```

## Troubleshooting

### Model Loading Issues

If you encounter issues with model loading:

```bash
# Remove and reinstall the model
ollama rm llama3
ollama pull llama3
```

### Connection Errors

If ResearchMate AI can't connect to Ollama:

1. Verify Ollama is running: `ps aux | grep ollama`
2. Check if the Ollama API is accessible: `curl http://localhost:11434/api/version`
3. Ensure no firewall is blocking the connection

## Resources

- [Ollama GitHub Repository](https://github.com/ollama/ollama)
- [Ollama Documentation](https://github.com/ollama/ollama/blob/main/README.md)
- [Llama 3 Information](https://ai.meta.com/blog/meta-llama-3/) 